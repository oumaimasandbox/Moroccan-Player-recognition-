{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction de caracteristiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/oumaima/Downloads/Player_em'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 73\u001b[0m\n\u001b[1;32m     70\u001b[0m     df\u001b[38;5;241m.\u001b[39mto_csv(csv_filename, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     72\u001b[0m root_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/oumaima/Downloads/Player_em\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Update this path to your dataset\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m data \u001b[38;5;241m=\u001b[39m process_images(root_folder)\n\u001b[1;32m     74\u001b[0m save_to_csv(data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/oumaima/Downloads/eqm_file.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 23\u001b[0m, in \u001b[0;36mprocess_images\u001b[0;34m(root_folder)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_images\u001b[39m(root_folder):\n\u001b[1;32m     21\u001b[0m     data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m folder_name \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(root_folder):\n\u001b[1;32m     24\u001b[0m         folder_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root_folder, folder_name)\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;66;03m# Check if the item in the directory is a subdirectory\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/oumaima/Downloads/Player_em'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "from skimage import feature\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "\n",
    "def extract_lbp_features(image):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Compute LBP features\n",
    "    lbp = feature.local_binary_pattern(gray, 8, 1, method=\"uniform\")\n",
    "    (hist, _) = np.histogram(lbp.ravel(), bins=np.arange(0, 10 + 3), range=(0, 10 + 2))\n",
    "    hist = hist.astype(\"float\")\n",
    "    hist /= (hist.sum() + 1e-7)\n",
    "    \n",
    "    return hist\n",
    "\n",
    "def process_images(root_folder):\n",
    "    data = []\n",
    "\n",
    "    for folder_name in os.listdir(root_folder):\n",
    "        folder_path = os.path.join(root_folder, folder_name)\n",
    "\n",
    "        # Check if the item in the directory is a subdirectory\n",
    "        if os.path.isdir(folder_path):\n",
    "            for filename in os.listdir(folder_path):\n",
    "                image_path = os.path.join(folder_path, filename)\n",
    "\n",
    "                # Ensure that the file is an image (you can customize this based on your image formats)\n",
    "                if image_path.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp')):\n",
    "                    # Read the image\n",
    "                    image = cv2.imread(image_path)\n",
    "\n",
    "                    # Check if the image is successfully read\n",
    "                    if image is not None:\n",
    "                        # Divide the image into blocks\n",
    "                        blocks = [image[i:i+10, j:j+10] for i, j in product(range(0, image.shape[0], 10), range(0, image.shape[1], 10))]\n",
    "\n",
    "                        # Initialize an array to hold the features for this image\n",
    "                        features_for_image = []\n",
    "\n",
    "                        # Extract LBP features for each block\n",
    "                        for block in blocks:\n",
    "                            lbp_features = extract_lbp_features(block)\n",
    "                            # Append the features for each block to the feature array\n",
    "                            features_for_image.extend(lbp_features)\n",
    "\n",
    "                        # Once all blocks are processed, append the features for the entire image to the data list\n",
    "                        data.append([folder_name] + features_for_image + [filename])\n",
    "                    else:\n",
    "                        print(f\"Error reading image: {image_path}\")\n",
    "                else:\n",
    "                    print(f\"Skipping non-image file: {image_path}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def save_to_csv(data, csv_filename):\n",
    "    # Determine the number of LBP features per image\n",
    "    num_features = len(data[0]) - 2  # subtract the folder name and filename\n",
    "    \n",
    "    # Create column names based on the number of features\n",
    "    columns =   [f'LBP_{i}' for i in range(num_features)] + ['ImageFile']+['ClassLabel']\n",
    "    \n",
    "    # Create a DataFrame with the correct column names\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "\n",
    "root_folder = '/Users/oumaima/Downloads/Player_em'  # Update this path to your dataset\n",
    "data = process_images(root_folder)\n",
    "save_to_csv(data, '/Users/oumaima/Downloads/eqm_file.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ClassLabel;LBP_1;LBP_2;LBP_3;LBP_4;LBP_5;LBP_6;LBP_7;LBP_8;LBP_9;LBP_10;LBP_11;LBP_12;LBP_13;LBP_14;LBP_15;LBP_16;LBP_17;LBP_18;LBP_19;LBP_20;LBP_21;LBP_22;LBP_23;LBP_24;LBP_25;LBP_26;LBP_27;LBP_28;LBP_29;LBP_30;LBP_31;LBP_32;LBP_33;LBP_34;LBP_35;LBP_36;LBP_37;LBP_38;LBP_39;LBP_40;LBP_41;LBP_42;LBP_43;LBP_44;LBP_45;LBP_46;LBP_47;LBP_48;LBP_49;LBP_50;LBP_51;LBP_52;LBP_53;LBP_54;LBP_55;LBP_56;LBP_57;LBP_58;LBP_59;LBP_60;LBP_61;LBP_62;LBP_63;LBP_64;LBP_65;LBP_66;LBP_67;LBP_68;LBP_69;LBP_70;LBP_71;LBP_72;LBP_73;LBP_74;LBP_75;LBP_76;LBP_77;LBP_78;LBP_79;LBP_80;LBP_81;LBP_82;LBP_83;LBP_84;LBP_85;LBP_86;LBP_87;LBP_88;LBP_89;LBP_90;LBP_91;LBP_92;LBP_93;LBP_94;LBP_95;LBP_96;LBP_97;LBP_98;LBP_99;LBP_100;LBP_101;LBP_102;LBP_103;LBP_104;LBP_105;LBP_106;LBP_107;LBP_108;LBP_109;LBP_110;LBP_111;LBP_112;LBP_113;LBP_114;LBP_115;LBP_116;LBP_117;LBP_118;LBP_119;LBP_120;LBP_121;LBP_122;LBP_123;LBP_124;LBP_125;LBP_126;LBP_127;LBP_128;LBP_129;LBP_130;LBP_131;LBP_132;LBP_133;LBP_134;LBP_135;LBP_136;LBP_137;LBP_138;LBP_139;LBP_140;LBP_141;LBP_142;LBP_143;LBP_144;LBP_145;LBP_146;LBP_147;LBP_148;LBP_149;LBP_150;LBP_151;LBP_152;LBP_153;LBP_154;LBP_155;LBP_156;LBP_157;LBP_158;LBP_159;LBP_160;LBP_161;LBP_162;LBP_163;LBP_164;LBP_165;LBP_166;LBP_167;LBP_168;LBP_169;LBP_170;LBP_171;LBP_172;LBP_173;LBP_174;LBP_175;LBP_176;LBP_177;LBP_178;LBP_179;LBP_180;LBP_181;LBP_182;LBP_183;LBP_184;LBP_185;LBP_186;LBP_187;LBP_188;LBP_189;LBP_190;LBP_191;LBP_192;LBP_193;LBP_194;LBP_195;LBP_196;LBP_197;LBP_198;LBP_199;LBP_200;LBP_201;LBP_202;LBP_203;LBP_204;LBP_205;LBP_206;LBP_207;LBP_208;LBP_209;LBP_210;LBP_211;LBP_212;LBP_213;LBP_214;LBP_215;LBP_216;LBP_217;LBP_218;LBP_219;LBP_220;LBP_221;LBP_222;LBP_223;LBP_224;LBP_225;LBP_226;LBP_227;LBP_228;LBP_229;LBP_230;LBP_231;LBP_232;LBP_233;LBP_234;LBP_235;LBP_236;LBP_237;LBP_238;LBP_239;LBP_240;LBP_241;LBP_242;LBP_243;LBP_244;LBP_245;LBP_246;LBP_247;LBP_248;LBP_249;LBP_250;LBP_251;LBP_252;LBP_253;LBP_254;LBP_255;LBP_256;LBP_257;LBP_258;LBP_259;LBP_260;LBP_261;LBP_262;LBP_263;LBP_264;LBP_265;LBP_266;LBP_267;LBP_268;LBP_269;LBP_270;LBP_271;LBP_272;LBP_273;LBP_274;LBP_275;LBP_276;LBP_277;LBP_278;LBP_279;LBP_280;LBP_281;LBP_282;LBP_283;LBP_284;LBP_285;LBP_286;LBP_287;LBP_288;LBP_289;LBP_290;LBP_291;LBP_292;LBP_293;LBP_294;LBP_295;LBP_296;LBP_297;LBP_298;LBP_299;LBP_300;LBP_301;LBP_302;LBP_303;LBP_304;LBP_305;LBP_306;LBP_307;LBP_308;LBP_309;LBP_310;LBP_311;LBP_312;LBP_313;LBP_314;LBP_315;LBP_316;LBP_317;LBP_318;LBP_319;LBP_320;LBP_321;LBP_322;LBP_323;LBP_324;LBP_325;LBP_326;LBP_327;LBP_328;LBP_329;LBP_330;LBP_331;LBP_332;LBP_333;LBP_334;LBP_335;LBP_336;LBP_337;LBP_338;LBP_339;LBP_340;LBP_341;LBP_342;LBP_343;LBP_344;LBP_345;LBP_346;LBP_347;LBP_348;LBP_349;LBP_350;LBP_351;LBP_352;LBP_353;LBP_354;LBP_355;LBP_356;LBP_357;LBP_358;LBP_359;LBP_360;LBP_361;LBP_362;LBP_363;LBP_364;LBP_365;LBP_366;LBP_367;LBP_368;LBP_369;LBP_370;LBP_371;LBP_372;LBP_373;LBP_374;LBP_375;LBP_376;LBP_377;LBP_378;LBP_379;LBP_380;LBP_381;LBP_382;LBP_383;LBP_384;LBP_385;LBP_386;LBP_387;LBP_388;LBP_389;LBP_390;LBP_391;LBP_392;LBP_393;LBP_394;LBP_395;LBP_396;LBP_397;LBP_398;LBP_399;LBP_400;LBP_401;LBP_402;LBP_403;LBP_404;LBP_405;LBP_406;LBP_407;LBP_408;LBP_409;LBP_410;LBP_411;LBP_412;LBP_413;LBP_414;LBP_415;LBP_416;LBP_417;LBP_418;LBP_419;LBP_420;LBP_421;LBP_422;LBP_423;LBP_424;LBP_425;LBP_426;LBP_427;LBP_428;LBP_429;LBP_430;LBP_431;LBP_432;LBP_433;LBP_434;LBP_435;LBP_436;LBP_437;LBP_438;LBP_439;LBP_440;LBP_441;LBP_442;LBP_443;LBP_444;LBP_445;LBP_446;LBP_447;LBP_448;LBP_449;LBP_450;LBP_451;LBP_452;LBP_453;LBP_454;LBP_455;LBP_456;LBP_457;LBP_458;LBP_459;LBP_460;LBP_461;LBP_462;LBP_463;LBP_464;LBP_465;LBP_466;LBP_467;LBP_468;LBP_469;LBP_470;LBP_471;LBP_472;LBP_473;LBP_474;LBP_475;LBP_476;LBP_477;LBP_478;LBP_479;LBP_480;LBP_481;LBP_482;LBP_483;LBP_484;LBP_485;LBP_486;LBP_487;LBP_488;LBP_489;LBP_490;LBP_491;LBP_492;LBP_493;LBP_494;LBP_495;LBP_496;LBP_497;LBP_498;LBP_499;LBP_500;LBP_501;LBP_502;LBP_503;LBP_504;LBP_505;LBP_506;LBP_507;LBP_508;LBP_509;LBP_510;LBP_511;LBP_512;LBP_513;LBP_514;LBP_515;LBP_516;LBP_517;LBP_518;LBP_519;LBP_520;LBP_521;LBP_522;LBP_523;LBP_524;LBP_525;LBP_526;LBP_527;LBP_528;LBP_529;LBP_530;LBP_531;LBP_532;LBP_533;LBP_534;LBP_535;LBP_536;LBP_537;LBP_538;LBP_539;LBP_540;LBP_541;LBP_542;LBP_543;LBP_544;LBP_545;LBP_546;LBP_547;LBP_548;LBP_549;LBP_550;LBP_551;LBP_552;LBP_553;LBP_554;LBP_555;LBP_556;LBP_557;LBP_558;LBP_559;LBP_560;LBP_561;LBP_562;LBP_563;LBP_564;LBP_565;LBP_566;LBP_567;LBP_568;LBP_569;LBP_570;LBP_571;LBP_572;LBP_573;LBP_574;LBP_575;LBP_576;LBP_577;LBP_578;LBP_579;LBP_580;LBP_581;LBP_582;LBP_583;LBP_584;LBP_585;LBP_586;LBP_587;LBP_588;LBP_589;LBP_590;LBP_591;LBP_592;LBP_593;LBP_594;LBP_595;LBP_596;LBP_597;LBP_598;LBP_599;LBP_600;LBP_601;LBP_602;LBP_603;LBP_604;LBP_605;LBP_606;LBP_607;LBP_608;LBP_609;LBP_610;LBP_611;LBP_612;LBP_613;LBP_614;LBP_615;LBP_616;LBP_617;LBP_618;LBP_619;LBP_620;LBP_621;LBP_622;LBP_623;LBP_624;LBP_625;LBP_626;LBP_627;LBP_628;LBP_629;LBP_630;LBP_631;LBP_632;LBP_633;LBP_634;LBP_635;LBP_636;LBP_637;LBP_638;LBP_639;LBP_640;LBP_641;LBP_642;LBP_643;LBP_644;LBP_645;LBP_646;LBP_647;LBP_648;LBP_649;LBP_650;LBP_651;LBP_652;LBP_653;LBP_654;LBP_655;LBP_656;LBP_657;LBP_658;LBP_659;LBP_660;LBP_661;LBP_662;LBP_663;LBP_664;LBP_665;LBP_666;LBP_667;LBP_668;LBP_669;LBP_670;LBP_671;LBP_672;LBP_673;LBP_674;LBP_675;LBP_676;LBP_677;LBP_678;LBP_679;LBP_680;LBP_681;LBP_682;LBP_683;LBP_684;LBP_685;LBP_686;LBP_687;LBP_688;LBP_689;LBP_690;LBP_691;LBP_692;LBP_693;LBP_694;LBP_695;LBP_696;LBP_697;LBP_698;LBP_699;LBP_700;LBP_701;LBP_702;LBP_703;LBP_704;LBP_705;LBP_706;LBP_707;LBP_708;LBP_709;LBP_710;LBP_711;LBP_712;LBP_713;LBP_714;LBP_715;LBP_716;LBP_717;LBP_718;LBP_719;LBP_720;LBP_721;LBP_722;LBP_723;LBP_724;LBP_725;LBP_726;LBP_727;LBP_728;LBP_729;LBP_730;LBP_731;LBP_732;LBP_733;LBP_734;LBP_735;LBP_736;LBP_737;LBP_738;LBP_739;LBP_740;LBP_741;LBP_742;LBP_743;LBP_744;LBP_745;LBP_746;LBP_747;LBP_748;LBP_749;LBP_750;LBP_751;LBP_752;LBP_753;LBP_754;LBP_755;LBP_756;LBP_757;LBP_758;LBP_759;LBP_760;LBP_761;LBP_762;LBP_763;LBP_764;LBP_765;LBP_766;LBP_767;LBP_768;LBP_769;LBP_770;LBP_771;LBP_772;LBP_773;LBP_774;LBP_775;LBP_776;LBP_777;LBP_778;LBP_779;LBP_780;LBP_781;LBP_782;LBP_783;LBP_784;LBP_785;LBP_786;LBP_787;LBP_788;LBP_789;LBP_790;LBP_791;LBP_792;LBP_793;LBP_794;LBP_795;LBP_796;LBP_797;LBP_798;LBP_799;LBP_800;LBP_801;LBP_802;LBP_803;LBP_804;LBP_805;LBP_806;LBP_807;LBP_808;LBP_809;LBP_810;LBP_811;LBP_812;LBP_813;LBP_814;LBP_815;LBP_816;LBP_817;LBP_818;LBP_819;LBP_820;LBP_821;LBP_822;LBP_823;LBP_824;LBP_825;LBP_826;LBP_827;LBP_828;LBP_829;LBP_830;LBP_831;LBP_832;LBP_833;LBP_834;LBP_835;LBP_836;LBP_837;LBP_838;LBP_839;LBP_840;LBP_841;LBP_842;LBP_843;LBP_844;LBP_845;LBP_846;LBP_847;LBP_848;LBP_849;LBP_850;LBP_851;LBP_852;LBP_853;LBP_854;LBP_855;LBP_856;LBP_857;LBP_858;LBP_859;LBP_860;LBP_861;LBP_862;LBP_863;LBP_864;LBP_865;LBP_866;LBP_867;LBP_868;LBP_869;LBP_870;LBP_871;LBP_872;LBP_873;LBP_874;LBP_875;LBP_876;LBP_877;LBP_878;LBP_879;LBP_880;LBP_881;LBP_882;LBP_883;LBP_884;LBP_885;LBP_886;LBP_887;LBP_888;LBP_889;LBP_890;LBP_891;LBP_892;LBP_893;LBP_894;LBP_895;LBP_896;LBP_897;LBP_898;LBP_899;LBP_900;LBP_901;LBP_902;LBP_903;LBP_904;LBP_905;LBP_906;LBP_907;LBP_908;LBP_909;LBP_910;LBP_911;LBP_912;LBP_913;LBP_914;LBP_915;LBP_916;LBP_917;LBP_918;LBP_919;LBP_920;LBP_921;LBP_922;LBP_923;LBP_924;LBP_925;LBP_926;LBP_927;LBP_928;LBP_929;LBP_930;LBP_931;LBP_932;LBP_933;LBP_934;LBP_935;LBP_936;LBP_937;LBP_938;LBP_939;LBP_940;LBP_941;LBP_942;LBP_943;LBP_944;LBP_945;LBP_946;LBP_947;LBP_948;LBP_949;LBP_950;LBP_951;LBP_952;LBP_953;LBP_954;LBP_955;LBP_956;LBP_957;LBP_958;LBP_959;LBP_960;LBP_961;LBP_962;LBP_963;LBP_964;LBP_965;LBP_966;LBP_967;LBP_968;LBP_969;LBP_970;LBP_971;LBP_972;LBP_973;LBP_974;LBP_975;LBP_976;LBP_977;LBP_978;LBP_979;LBP_980;LBP_981;LBP_982;LBP_983;LBP_984;LBP_985;LBP_986;LBP_987;LBP_988;LBP_989;LBP_990;LBP_991;LBP_992;LBP_993;LBP_994;LBP_995;LBP_996;LBP_997;LBP_998;LBP_999']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "column_name = [col for col in lbp_data.columns if re.search('classlabel', col, re.I)]\n",
    "print(column_name)  # This should print the column name that matches the pattern\n",
    "lbp_data = pd.read_csv('/Users/oumaima/Downloads/eqm_file.csv')\n",
    "# Then use the exact column name found\n",
    "y = lbp_data[column_name[0]]  # Access the column using the matched name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modele Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Shared/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 54.69%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load LBP features and class labels from CSV\n",
    "lbp_data = pd.read_csv('/Users/oumaima/Downloads/eqm_file.csv', delimiter=';')\n",
    "# Strip leading and trailing spaces from column names\n",
    "# Normalize and encode the column names\n",
    "\n",
    "\n",
    "# Extract features and labels\n",
    "X = lbp_data.iloc[:, 1:]\n",
    "y = lbp_data['ClassLabel']\n",
    "\n",
    "# Convert class labels to numerical format\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize a RandomForestClassifier as the base estimator\n",
    "rf_classifier = RandomForestClassifier(n_estimators=300, random_state=42)\n",
    "\n",
    "# Initialize a BaggingClassifier with the RandomForestClassifier as the base estimator\n",
    "bagging_classifier = BaggingClassifier(base_estimator=rf_classifier, n_estimators=50, random_state=42)\n",
    "\n",
    "# Train the bagging classifier\n",
    "bagging_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = bagging_classifier.predict(X_test)\n",
    "\n",
    "# Decode the predicted labels back to original class labels\n",
    "y_pred_original = label_encoder.inverse_transform(y_pred)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modele SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 10, 'kernel': 'rbf'}\n",
      "Accuracy on the test set with best model: 0.76\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "df = pd.read_csv('/Users/oumaima/Downloads/eqm_file.csv', delimiter=';')\n",
    "\n",
    "# Convert LBP features from string to a numeric array only if they are strings\n",
    "for col in df.columns[1:]:  # Skip 'ClassLabel' column\n",
    "    # Check if the column data type is string\n",
    "    if df[col].dtype == object:\n",
    "        # Convert from string to numeric array\n",
    "        df[col] = df[col].apply(lambda x: np.fromstring(x, sep=' ') if isinstance(x, str) else x)\n",
    "\n",
    "# Extract features and labels\n",
    "X = df.iloc[:, 1:]  # Features\n",
    "y = df['ClassLabel']  # Labels\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define parameters for grid search\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],  # Example range, adjust as needed\n",
    "    'kernel': ['linear', 'rbf'],  # Example kernels, adjust as needed\n",
    "    # Add other parameters as needed\n",
    "}\n",
    "\n",
    "# Create an SVM model\n",
    "svm_model = SVC()\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search = GridSearchCV(svm_model, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters and the corresponding best model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set with the best model\n",
    "y_test_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Print the best parameters\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "# Print the accuracy on the test set\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Accuracy on the test set with best model: {accuracy:.2f}\")\n",
    "joblib.dump(scaler, 'scaler.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
